# -*- coding: utf-8 -*-
"""try0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I_sV1WjJke4r29qJBirNfpbqr-ogBiKt
"""
#from imutils import paths
import os
import argparse
import numpy as np
import pylab as plt
from glob import glob
from matplotlib import cm
from skimage.io import imread
from skimage.transform import resize

from tensorflow.keras import layers
from tensorflow.keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.activations import linear
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.preprocessing.image import ImageDataGenerator


from tf_keras_vis.utils import normalize
from tf_keras_vis.gradcam import Gradcam
from tf_keras_vis.saliency import Saliency
from tf_keras_vis.utils import num_of_gpus


parser = argparse.ArgumentParser(description='MODEL ACTIVITY ANALYZER.')
parser.add_argument('--dataset', default='./dataset', type=str, help='path to dataset')
parser.add_argument('--model', default='model file name', type=str, help='model file name')
parser.add_argument('--lx', default=128, type=int, help='image length')
parser.add_argument('--ly', default=128, type=int, help='image width')
parser.add_argument('--n_sample', default=4, type=int, help='number of sample')
parser.add_argument('--epochs', default=200, type=int, help='number of epochs')
parser.add_argument('--BS', default=32, type=int, help='number of epochs')

parser.add_argument('--dpi', default=200, type=int, help='image dpi')
parser.add_argument('--restart', action="store_true")

args = parser.parse_args()
data_path = args.dataset
lx,ly = args.lx,args.ly
n_sample = args.n_sample
restart = args.restart
EPOCHS = args.epochs
BS = args.BS
dpi = args.dpi

paths = glob(data_path+'/*')
#labels = [i.split('/')[-1] for i in paths]

dataste_name = 'data_set'

print("[INFO] reading data and preparation...")
if not os.path.isfile(dataste_name+'.npz') or restart:
    x = []
    labels = []
    for path in paths:
        files = glob(path+'/*')
        for fil in files:
            try:
                img = imread(fil)
                img = resize(img,output_shape=(lx,ly))
                if img.ndim==3:
                    img = np.mean(img,axis=-1)
                x.append(img)
                labels.append(fil.split('/')[-2])
            except:
                print('Something is wrong with',fil,', skipped.')
    print("[INFO] prepared data is saved.")
    np.savez('data_set',x=x,labels=labels)
else:
    data = np.load('data_set.npz')
    x = data['x']
    labels = data['labels']

class_labels, nums = np.unique(labels,return_counts=True)
n_calss = len(class_labels)
print('labels/numbers are:\n',*['{:15s}/{:10d}\n'.format(i,j) for i,j in zip(class_labels,nums)])

dummy = {j: i for i,j in enumerate(labels)}
int_map = {}
lbl_map = {}
for j,(i,_) in enumerate(dummy.items()):
  int_map[i] = j
  lbl_map[j] = i

vec = [int_map[word] for word in labels]
x = np.array(x)
vec = np.array(vec)
y = to_categorical(vec, num_classes=None, dtype='float32')
n_data,lx0,ly0 = x.shape
x = x[:,:,:,None]

# x = np.stack(3*[x],axis=-1)

print('[INFO] input/output shapes: {}/{}'.format(x.shape,y.shape))

imgs = []
for i in range(n_calss):
    imgs.append(x[vec==i][:n_sample])

imgs = np.array(imgs)
subplot_args = { 'nrows': n_sample, 'ncols': n_calss,
                 'figsize': (int(4*n_calss),int( 5*n_sample)),
                 'subplot_kw': {'xticks': [], 'yticks': []} }

subplot_args2 = { 'nrows': 1, 'ncols': n_calss,
                 'figsize': (int(4*n_calss),5),
                 'subplot_kw': {'xticks': [], 'yticks': []} }

f, ax = plt.subplots(**subplot_args)
for i in range(n_calss):
  for j in range(n_sample):
    title = lbl_map[i]
    ax[j,i].set_title(title, fontsize=40)
    ax[j,i].imshow(imgs[i,j,:,:,0],cmap='gray')
plt.tight_layout()
plt.savefig('samples.jpg',dpi=150)
plt.close()

if not os.path.isfile('model.h5') or restart:
    print('[INFO] building neural net...')
    # initialize the initial learning rate, number of epochs to train for,
    # and batch size
    INIT_LR = 1e-3

    def lr_scheduler(epoch, lr):
        return lr * 0.985

    callbacks = [
        LearningRateScheduler(lr_scheduler, verbose=1)
    ]
    
    # initialize the training data augmentation object
    trainAug = ImageDataGenerator(
	    rotation_range=15,
	    fill_mode="nearest")

    inp = layers.Input(shape=(lx,ly,1), name="img")
    xl = layers.Conv2D(16, 3, activation="relu")(inp)
    xl = layers.Conv2D(32, 3, activation="relu")(xl)
    xl = layers.MaxPooling2D(3)(xl)
    xl = layers.Conv2D(32, 3, activation="relu")(xl)
    xl = layers.Conv2D(16, 3, activation="relu")(xl)
    xl = layers.MaxPooling2D(3)(xl)
#    xl = layers.Conv2D(16, 3, activation="relu")(xl)
#    xl = layers.Conv2D(16, 3, activation="relu")(xl)
#    xl = layers.MaxPooling2D(3)(xl)
    xl = layers.Conv2D(16, 3, activation="relu")(xl)
    xl = layers.Conv2D(8, 3, activation="relu")(xl)
    xl = layers.Conv2D(16, 3, activation="relu")(xl)
    xl = layers.Conv2D(8, 3, activation="relu")(xl)
    xl = layers.Flatten(name="flatten")(xl)
    xl = layers.Dense(64, activation="relu")(xl)
    xl = layers.Dropout(0.5)(xl)
    yl = layers.Dense(n_calss, activation="softmax")(xl)

    model = Model(inputs=inp, outputs=yl)

    model.summary()

    # compile our model
    print("[INFO] compiling model...")
    opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)
    model.compile(loss="binary_crossentropy", optimizer=opt,
	    metrics=["accuracy"])

    # train the head of the network
    print("[INFO] training ...")
    H = model.fit_generator(
	    trainAug.flow(x, y, batch_size=BS),
	    steps_per_epoch=len(x) // BS,
	    epochs=EPOCHS,
	    callbacks=callbacks)

    model.save('model.h5', save_format="h5")
    print('[INFO] trained model is saved.')

else:
#_, gpus = num_of_gpus()
#print('{} GPUs'.format(gpus))
    model = load_model('model.h5')
    print('[INFO] trained model is loaded.')
    print('[INFO] model summary is:\n\n')
    model.summary()
    print('\n\n\n')
    
def loss(output):
    return output

def model_modifier(m):
    m.layers[-1].activation = linear
    return m


"""# Vanilla SaliencyÂ¶ """
saliency = Saliency(model,
                    model_modifier=model_modifier,
                    clone=False)

saliency_maps = []
for i in range(n_calss):
    imgsp = imgs[i]
    saliency_map = saliency(loss, imgsp)
    saliency_map = normalize(saliency_map)
    saliency_maps.append(saliency_map)
saliency_maps = np.array(saliency_maps)

f, ax = plt.subplots(**subplot_args)
for i in range(n_calss):
  for j in range(n_sample):
    title = lbl_map[i]
    ax[j,i].set_title(title, fontsize=40)
    ax[j,i].imshow(imgs[i,j,:,:,0],cmap='gray')
    ax[j,i].imshow(saliency_maps[i,j], cmap='jet',alpha=0.4)
plt.tight_layout()
plt.savefig('vanilla_saliency_samples.jpg',dpi=dpi)
plt.close()

saliency_maps = saliency(loss, x)
saliency_maps = normalize(saliency_maps)

f, ax = plt.subplots(**subplot_args2)
for i in range(n_calss):
    title = lbl_map[i]
    ax[i].set_title(title, fontsize=40)
    ax[i].imshow(np.mean(saliency_maps[vec==i],axis=0), cmap='jet')
plt.tight_layout()
plt.savefig('vanilla_saliency_avg.jpg',dpi=dpi)
plt.close()


"""# SmoothGrad"""
model = load_model('model.h5')
saliency = Saliency(model,
                    model_modifier=model_modifier,
                    clone=False)
saliency_maps = []
for i in range(n_calss):
    imgsp = imgs[i]
    saliency_map = saliency(loss, imgsp,
                            smooth_samples=20, # The number of calculating gradients iterations.
                            smooth_noise=0.20)
    saliency_map = normalize(saliency_map)
    saliency_maps.append(saliency_map)
saliency_maps = np.array(saliency_maps)

f, ax = plt.subplots(**subplot_args)
for i in range(n_calss):
  for j in range(n_sample):
    title = lbl_map[i]
    ax[j,i].set_title(title, fontsize=40)
    ax[j,i].imshow(imgs[i,j,:,:,0],cmap='gray')
    ax[j,i].imshow(saliency_maps[i,j], cmap='jet',alpha=0.4)
plt.tight_layout()
plt.savefig('smoothgrad_samples.jpg',dpi=dpi)
plt.close()

saliency_maps = saliency(loss, x,
                         smooth_samples=20, # The number of calculating gradients iterations.
                         smooth_noise=0.20)
saliency_maps = normalize(saliency_maps)

f, ax = plt.subplots(**subplot_args2)
for i in range(n_calss):
    title = lbl_map[i]
    ax[i].set_title(title, fontsize=40)
    ax[i].imshow(np.mean(saliency_maps[vec==i],axis=0), cmap='jet')
plt.tight_layout()
plt.savefig('smoothgrad_avg.jpg',dpi=dpi)
plt.close()


#"""# GradCAM"""
#model = load_model('model.h5')
#gradcam = Gradcam(model,
#                  model_modifier=model_modifier,
#                  clone=False)

#saliency_maps = []
#for i in range(n_calss):
#    imgsp = imgs[i]
#    saliency_map = gradcam(loss,imgsp,
#                           penultimate_layer=-1 # model.layers number
#                           )
#    saliency_map = normalize(saliency_map)
#    saliency_maps.append(saliency_map)
#saliency_maps = np.array(saliency_maps)

#f, ax = plt.subplots(**subplot_args)
#for i in range(n_calss):
#  for j in range(n_sample):
#    title = lbl_map[i]
#    ax[j,i].set_title(title, fontsize=40)
#    ax[j,i].imshow(imgs[i,j,:,:,0],cmap='gray')
#    heatmap = np.uint8(cm.jet(saliency_maps[i,j])[..., :3] * 255)
#    ax[j,i].imshow(heatmap, cmap='jet',alpha=0.4)
#plt.tight_layout()
#plt.savefig('gradCAM_samples.jpg',dpi=dpi)
#plt.close()

#saliency_maps = gradcam(loss,x,
#                           penultimate_layer=-1 # model.layers number
#                           )
#saliency_maps = normalize(saliency_maps)

#f, ax = plt.subplots(**subplot_args2)
#for i in range(n_calss):
#    title = lbl_map[i]
#    ax[i].set_title(title, fontsize=40)
#    salm = np.mean(saliency_maps[vec==i],axis=0)
#    heatmap = np.uint8(cm.jet(salm)[..., :3] * 255)
#    ax[i].imshow(heatmap, cmap='jet')
#plt.tight_layout()
#plt.savefig('gradCAM_avg.jpg',dpi=dpi)
#plt.close()


print('[INFO] Done!')




